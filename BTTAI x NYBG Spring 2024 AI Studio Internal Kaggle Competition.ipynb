{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a2d086",
   "metadata": {},
   "source": [
    "### BTTAI x NYBG Spring 2024 AI Studio Internal Kaggle Competition\n",
    "#### *Team Sweet Pea_BOS(Muya Guoji & Katie Boscombe)*\n",
    "The aim of this project is to advance biodiversity research by building an ML model to categorize plant specimen images, for the New York Botanical Garden. Details about the original competition can be found here: `(https://www.kaggle.com/competitions/bttai-nybg-2024)`. Our final model accuracy rate for the test data in the competition is 98.44%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad6b40",
   "metadata": {},
   "source": [
    "#### 1. Import Libraries and Modules\n",
    "We start by importing necessary Python libraries and modules that will be used in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.applications.resnet import ResNet101, preprocess_input as preprocess_input_101, decode_predictions as decode_predictions_101\n",
    "from keras.applications.resnet_v2 import ResNet152V2, preprocess_input as preprocess_input_152, decode_predictions as decode_predictions_152\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae67ce7",
   "metadata": {},
   "source": [
    "#### 2. Data Preparation\n",
    "\n",
    "Data is then loaded from CSV files containing training, validation, and test sets. The directory paths for images corresponding to each set are specified in the original kaggle competition. DataFrames are manipulated to ensure class IDs are read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676f4a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T02:52:55.072019Z",
     "iopub.status.busy": "2024-04-08T02:52:55.071468Z",
     "iopub.status.idle": "2024-04-08T02:52:55.333986Z",
     "shell.execute_reply": "2024-04-08T02:52:55.332957Z"
    },
    "papermill": {
     "duration": 0.270529,
     "end_time": "2024-04-08T02:52:55.336241",
     "exception": false,
     "start_time": "2024-04-08T02:52:55.065712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train.csv\")\n",
    "train_dir = \"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train/BTTAIxNYBG-train\"\n",
    "validation_df = pd.read_csv(\"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation.csv\")\n",
    "validation_dir = \"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation/BTTAIxNYBG-validation\"\n",
    "test_df = pd.read_csv(\"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test.csv\")\n",
    "test_dir = \"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test/BTTAIxNYBG-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb731a",
   "metadata": {},
   "source": [
    "#### 3. Image Data Preprocessing and Augmentation\n",
    "\n",
    "Data augmentation is a technique used to artificially expand the diversity of a dataset by applying random, yet realistic, transformations to the training images. This process aims to prevent the model from overfitting and aids in generalizing better to new, unseen data. We implemented a few specific augmentation techniques applied to the training dataset:\n",
    "\n",
    "*Rescaling*: Each pixel in the image is rescaled by a factor of 1/255. This transformation converts the pixel values from a range of 0-255 to 0-1, making the neural network's task of learning from these data easier and more stable.\n",
    "\n",
    "*Horizontal Flip*: Images are randomly flipped horizontally. This augmentation assumes that a horizontal flip does not change the semantic meaning of the image, which is generally a safe assumption in natural scenes and objects like plants.\n",
    "\n",
    "*Rotation*: Images are randomly rotated by a degree in the range of 0 to 20 degrees. This mimics the variation in orientation that can occur with camera angles or object placement, enhancing the robustness of the classifier.\n",
    "\n",
    "*Shear Transformation*: A shear transformation slants the shape of an image, simulating a change in viewing angle. Here, a shear intensity of 0.1 (shear angle in degrees) introduces slight distortions in the image geometry.\n",
    "\n",
    "Each of these transformations introduces variability into the dataset but retains the essential features necessary for accurate classification, to  equip the model to handle real-world variability in inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f09342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T02:52:55.346841Z",
     "iopub.status.busy": "2024-04-08T02:52:55.346443Z",
     "iopub.status.idle": "2024-04-08T02:52:57.929734Z",
     "shell.execute_reply": "2024-04-08T02:52:57.928936Z"
    },
    "papermill": {
     "duration": 2.591081,
     "end_time": "2024-04-08T02:52:57.932072",
     "exception": false,
     "start_time": "2024-04-08T02:52:55.340991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['classID'] = train_df['classID'].astype(str)\n",
    "validation_df['classID'] = validation_df['classID'].astype(str)\n",
    "items = os.listdir(train_dir)\n",
    "num_items = len(items)\n",
    "\n",
    "def pre_processing():\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=20,\n",
    "        shear_range=0.1,  # Shear intensity (shear angle in degrees)\n",
    "    )\n",
    "    return datagen\n",
    "\n",
    "datagen = pre_processing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4537a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T02:52:57.942926Z",
     "iopub.status.busy": "2024-04-08T02:52:57.942254Z",
     "iopub.status.idle": "2024-04-08T02:58:12.403230Z",
     "shell.execute_reply": "2024-04-08T02:58:12.402458Z"
    },
    "papermill": {
     "duration": 314.468778,
     "end_time": "2024-04-08T02:58:12.405598",
     "exception": false,
     "start_time": "2024-04-08T02:52:57.936820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81946 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    directory = train_dir,\n",
    "    x_col = \"imageFile\",\n",
    "    y_col = \"classID\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b911885",
   "metadata": {},
   "source": [
    "As for validation data processing, the only transformation applied to the validation data is to rescale it by a factor of 1/255, stabilizes the data values from a range of 0-255 to 0-1, just like the training dataset, so that the performance is assessed on minimally altered images, maintaining the integrity and reliability of the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce5ded8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T02:58:12.416710Z",
     "iopub.status.busy": "2024-04-08T02:58:12.415995Z",
     "iopub.status.idle": "2024-04-08T02:58:53.338645Z",
     "shell.execute_reply": "2024-04-08T02:58:53.337733Z"
    },
    "papermill": {
     "duration": 40.931995,
     "end_time": "2024-04-08T02:58:53.342393",
     "exception": false,
     "start_time": "2024-04-08T02:58:12.410398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10244 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# For the validation data, only apply rescaling\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=validation_df,\n",
    "    directory=validation_dir,\n",
    "    x_col=\"imageFile\",\n",
    "    y_col=\"classID\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaedcf7",
   "metadata": {},
   "source": [
    "#### 4. Model building \n",
    "A ResNet101 base model loaded with pretrained ImageNet weights is used to classify the images. We implemented additional layers to allow the model to transition from understanding generic image features (learned from ImageNet) to interpreting and classifying images based on the specific characteristics of the NYBG image dataset. They adapt the model from broad image recognition tasks to a focused classification problem, enabling it to perform with higher accuracy and relevance to the target task. \n",
    "\n",
    "1. GlobalAveragePooling2D:\n",
    "This layer is used to reduce the spatial dimensions of the input feature maps. Instead of flattening the feature maps and potentially losing some of the spatial features, global average pooling computes the average value for each feature map, retaining the most essential signal of each feature with fewer parameters. This helps in reducing the model complexity and the risk of overfitting, making the network more efficient.\n",
    "\n",
    "2. Dense Layer (1024 units, ReLU activation):\n",
    "After the feature reduction through pooling, the model needs to learn which features are most relevant in distinguishing between the different classes specific to the NYBG dataset. A dense layer with 1024 neurons and ReLU activation is introduced to serve this purpose. This layer provides the model with a substantial amount of trainable parameters, allowing it to learn complex patterns deeply from the pooled features.\n",
    "\n",
    "3. Output Dense Layer (10 units, Softmax activation):\n",
    "The final layer in the architecture is a dense layer with a number of units equal to the number of classes to be predicted (in this case, 10). This layer uses the softmax activation function to output a probability distribution across the 10 classes, where each value represents the likelihood of the input image belonging to one of the classes. This layer essentially maps the learned features to specific class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99720dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T02:58:53.353016Z",
     "iopub.status.busy": "2024-04-08T02:58:53.352715Z",
     "iopub.status.idle": "2024-04-08T02:59:05.571097Z",
     "shell.execute_reply": "2024-04-08T02:59:05.570308Z"
    },
    "papermill": {
     "duration": 12.22615,
     "end_time": "2024-04-08T02:59:05.573330",
     "exception": false,
     "start_time": "2024-04-08T02:58:53.347180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m171446536/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "#resnet101 model\n",
    "base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add additional layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda659a",
   "metadata": {},
   "source": [
    "#### 5. Training and Callbacks\n",
    "\n",
    "The model's training incorporates callbacks for early stopping and learning rate reduction, both of which are contingent on the validation loss to optimize training efficiency. The learning rate reduction callback activates when there is no improvement in a monitored metric, specifically adjusting the learning rate downward. This adjustment facilitates more precise training by enabling the model to make smaller, more deliberate updates in its parameters, particularly useful when progress stalls or reaches a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46976449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T02:59:05.602521Z",
     "iopub.status.busy": "2024-04-08T02:59:05.601804Z",
     "iopub.status.idle": "2024-04-08T12:38:35.613501Z",
     "shell.execute_reply": "2024-04-08T12:38:35.611144Z"
    },
    "papermill": {
     "duration": 34772.888194,
     "end_time": "2024-04-08T12:38:38.476002",
     "exception": false,
     "start_time": "2024-04-08T02:59:05.587808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712545274.021778      70 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2867s\u001b[0m 1s/step - accuracy: 0.8270 - loss: 0.5556 - val_accuracy: 0.7896 - val_loss: 0.6426 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2027s\u001b[0m 790ms/step - accuracy: 0.9367 - loss: 0.1870 - val_accuracy: 0.8614 - val_loss: 0.4091 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2239s\u001b[0m 872ms/step - accuracy: 0.9482 - loss: 0.1525 - val_accuracy: 0.8526 - val_loss: 0.5000 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2174s\u001b[0m 847ms/step - accuracy: 0.9551 - loss: 0.1304 - val_accuracy: 0.8995 - val_loss: 0.3210 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2338s\u001b[0m 911ms/step - accuracy: 0.9611 - loss: 0.1152 - val_accuracy: 0.8263 - val_loss: 0.6602 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2251s\u001b[0m 877ms/step - accuracy: 0.9651 - loss: 0.1003 - val_accuracy: 0.8135 - val_loss: 0.6452 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2222s\u001b[0m 865ms/step - accuracy: 0.9687 - loss: 0.0943 - val_accuracy: 0.9081 - val_loss: 0.3242 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2329s\u001b[0m 907ms/step - accuracy: 0.9800 - loss: 0.0568 - val_accuracy: 0.9749 - val_loss: 0.0831 - learning_rate: 2.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2303s\u001b[0m 897ms/step - accuracy: 0.9843 - loss: 0.0433 - val_accuracy: 0.9776 - val_loss: 0.0759 - learning_rate: 2.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2345s\u001b[0m 913ms/step - accuracy: 0.9870 - loss: 0.0374 - val_accuracy: 0.9778 - val_loss: 0.0777 - learning_rate: 2.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2308s\u001b[0m 899ms/step - accuracy: 0.9886 - loss: 0.0337 - val_accuracy: 0.9747 - val_loss: 0.0860 - learning_rate: 2.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2283s\u001b[0m 889ms/step - accuracy: 0.9893 - loss: 0.0308 - val_accuracy: 0.9793 - val_loss: 0.0685 - learning_rate: 2.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2360s\u001b[0m 919ms/step - accuracy: 0.9902 - loss: 0.0283 - val_accuracy: 0.9633 - val_loss: 0.1188 - learning_rate: 2.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2406s\u001b[0m 937ms/step - accuracy: 0.9909 - loss: 0.0256 - val_accuracy: 0.9747 - val_loss: 0.0879 - learning_rate: 2.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m2561/2561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2316s\u001b[0m 902ms/step - accuracy: 0.9916 - loss: 0.0234 - val_accuracy: 0.9746 - val_loss: 0.0951 - learning_rate: 2.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b63f4425ba0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# early stopping \n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,  # Wait for 5 epochs with no improvement\n",
    "    restore_best_weights=True,  # Restore model weights to the best observed\n",
    "    verbose=1  # Show messages about early stopping\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_generator, \n",
    "    epochs=15,\n",
    "    callbacks=[callback, reduce_lr],\n",
    "    validation_data=validation_generator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3cc297",
   "metadata": {},
   "source": [
    "\n",
    "After training the neural network, it is crucial to save the trained model to allow for later use without the need to retrain from scratch (model serialization). This process is handled in this segment of the code using TensorFlow's Keras API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b35919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:38:45.020157Z",
     "iopub.status.busy": "2024-04-08T12:38:45.019087Z",
     "iopub.status.idle": "2024-04-08T12:38:49.960298Z",
     "shell.execute_reply": "2024-04-08T12:38:49.959211Z"
    },
    "papermill": {
     "duration": 8.106606,
     "end_time": "2024-04-08T12:38:49.964235",
     "exception": false,
     "start_time": "2024-04-08T12:38:41.857629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model \n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model.save(\"/kaggle/working/model_\" + current_datetime + \".h5\")\n",
    "# save model again \n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model.save(\"/kaggle/working/model_\" + current_datetime + \".keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4fb19",
   "metadata": {},
   "source": [
    "#### 6. Loading & Testing the Model with Test Data\n",
    "\n",
    "Import the test data into the trained model to generate the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60cb92c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:38:56.693261Z",
     "iopub.status.busy": "2024-04-08T12:38:56.691430Z",
     "iopub.status.idle": "2024-04-08T12:40:54.631504Z",
     "shell.execute_reply": "2024-04-08T12:40:54.630271Z"
    },
    "papermill": {
     "duration": 124.406374,
     "end_time": "2024-04-08T12:40:57.760503",
     "exception": false,
     "start_time": "2024-04-08T12:38:53.354129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30690 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test.csv\")\n",
    "test_dir = \"/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-test/BTTAIxNYBG-test\"\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    directory = test_dir,\n",
    "    x_col=\"imageFile\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None, \n",
    "    shuffle=False  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86ec0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:41:04.369210Z",
     "iopub.status.busy": "2024-04-08T12:41:04.368847Z",
     "iopub.status.idle": "2024-04-08T12:56:15.734908Z",
     "shell.execute_reply": "2024-04-08T12:56:15.733912Z"
    },
    "papermill": {
     "duration": 914.679075,
     "end_time": "2024-04-08T12:56:15.737195",
     "exception": false,
     "start_time": "2024-04-08T12:41:01.058120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 939ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a8aeaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:56:22.414920Z",
     "iopub.status.busy": "2024-04-08T12:56:22.413990Z",
     "iopub.status.idle": "2024-04-08T12:56:22.420414Z",
     "shell.execute_reply": "2024-04-08T12:56:22.419496Z"
    },
    "papermill": {
     "duration": 3.229779,
     "end_time": "2024-04-08T12:56:22.422351",
     "exception": false,
     "start_time": "2024-04-08T12:56:19.192572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = test_df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb7cd78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:56:29.358791Z",
     "iopub.status.busy": "2024-04-08T12:56:29.357680Z",
     "iopub.status.idle": "2024-04-08T12:56:29.371231Z",
     "shell.execute_reply": "2024-04-08T12:56:29.370437Z"
    },
    "papermill": {
     "duration": 3.496614,
     "end_time": "2024-04-08T12:56:29.373914",
     "exception": false,
     "start_time": "2024-04-08T12:56:25.877300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df['predicted_class'] = \"\"\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "results_df['predicted_class'] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c5e720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:56:36.106688Z",
     "iopub.status.busy": "2024-04-08T12:56:36.106193Z",
     "iopub.status.idle": "2024-04-08T12:56:36.208023Z",
     "shell.execute_reply": "2024-04-08T12:56:36.207147Z"
    },
    "papermill": {
     "duration": 3.497377,
     "end_time": "2024-04-08T12:56:36.210119",
     "exception": false,
     "start_time": "2024-04-08T12:56:32.712742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>imageFile</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>facd4dcd8e869617.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>78c96bb2b2b62579.jpg</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>d292d2c4e0e6ad9d.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>3633494929870713.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>dc94b496c8e2d6c4.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30685</th>\n",
       "      <td>122864</td>\n",
       "      <td>9ab2ba9a949abab2.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30686</th>\n",
       "      <td>122868</td>\n",
       "      <td>ccccede8cccccc4f.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30687</th>\n",
       "      <td>122871</td>\n",
       "      <td>31ccec6c99ccec68.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30688</th>\n",
       "      <td>122878</td>\n",
       "      <td>de1e0f1f0e0e9e9e.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30689</th>\n",
       "      <td>122880</td>\n",
       "      <td>1e3b352565b3b687.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30690 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uniqueID             imageFile  predicted_class\n",
       "0             1  facd4dcd8e869617.jpg                1\n",
       "1             9  78c96bb2b2b62579.jpg                9\n",
       "2            10  d292d2c4e0e6ad9d.jpg                4\n",
       "3            14  3633494929870713.jpg                1\n",
       "4            16  dc94b496c8e2d6c4.jpg                6\n",
       "...         ...                   ...              ...\n",
       "30685    122864  9ab2ba9a949abab2.jpg                7\n",
       "30686    122868  ccccede8cccccc4f.jpg                1\n",
       "30687    122871  31ccec6c99ccec68.jpg                0\n",
       "30688    122878  de1e0f1f0e0e9e9e.jpg                1\n",
       "30689    122880  1e3b352565b3b687.jpg                2\n",
       "\n",
       "[30690 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cbb877b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T12:56:42.833903Z",
     "iopub.status.busy": "2024-04-08T12:56:42.833517Z",
     "iopub.status.idle": "2024-04-08T12:56:42.892193Z",
     "shell.execute_reply": "2024-04-08T12:56:42.891470Z"
    },
    "papermill": {
     "duration": 3.43371,
     "end_time": "2024-04-08T12:56:42.894190",
     "exception": false,
     "start_time": "2024-04-08T12:56:39.460480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df = results_df[['uniqueID','predicted_class']].copy(deep=True)\n",
    "submit_df.rename(columns={'predicted_class': 'classID'}, inplace=True)\n",
    "submit_df.to_csv('submission' + current_datetime + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7522884,
     "sourceId": 66199,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36252.306812,
   "end_time": "2024-04-08T12:56:50.966050",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T02:52:38.659238",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
